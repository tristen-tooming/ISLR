---
title: "Lab3"
author: "Tristen Tooming"
date: "2/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


###Assignment 1: Analysis of mortality rates using splines

##### Pre
```{r}
mortality_rate = read.csv('mortality_rate.csv', header = TRUE, sep = ';')
summary(mortality_rate$Rate)
```

##### 1.a.

```{r}
mortality_rate$lmr = log(mortality_rate$Rate)
plot(mortality_rate$Day, mortality_rate$lmr,
     ylab = 'LMR',
     xlab = 'Day',
     main = 'Log-Mortality-Rate (LMR)')
```
<br>
> Plot looks a little bit scrappy and you cannot see a solid 'line' to indicate that data follows an exponential mortality rate. 

###### 1.b

```{r}
library(splines)

fit_ns_1 = lm(lmr ~ ns(Day, df = 2), data = mortality_rate)
pred1 = predict(fit_ns_1, newdata = list(Day=mortality_rate$Day))

fit_ns_2 = lm(lmr ~ ns(Day, df = 3), data = mortality_rate)
pred2 = predict(fit_ns_2, newdata = list(Day=mortality_rate$Day))

fit_ns_3 = lm(lmr ~ ns(Day, df = 16), data = mortality_rate)
pred3 = predict(fit_ns_3, newdata = list(Day=mortality_rate$Day))

fit_ns_4 = lm(lmr ~ ns(Day, df = 51), data = mortality_rate)
pred4 = predict(fit_ns_4, newdata = list(Day=mortality_rate$Day))
```


```{r, echo=F}
plot(mortality_rate$Day, mortality_rate$lmr, xlab = 'Day', ylab = 'LMR',
     main = 'Natural cubic splies fit #1 (knots = 1)')
lines(mortality_rate$Day, pred1, col='black', lwd = 2)
points(attr(ns(mortality_rate$Day, df = 2), "knots"),
  predict(fit_ns_1,
          data.frame(Day = attr(ns(mortality_rate$Day, df = 2), "knots"))),
  col="black",pch=4, lwd = 3, cex = 1)

plot(mortality_rate$Day, mortality_rate$lmr, xlab = 'Day', ylab = 'LMR',
     main = 'Natural cubic splies fit #2 (knots = 2)')
lines(mortality_rate$Day, pred2, col='blue', lwd = 2)
points(attr(ns(mortality_rate$Day, df = 3), "knots"),
  predict(fit_ns_2,
          data.frame(Day = attr(ns(mortality_rate$Day, df = 3), "knots"))),
  col="blue",pch=4, lwd = 3, cex = 1)

plot(mortality_rate$Day, mortality_rate$lmr, xlab = 'Day', ylab = 'LMR',
     main = 'Natural cubic splies fit #3 (knots = 15)')
lines(mortality_rate$Day, pred3, col='green', lwd = 2)
points(attr(ns(mortality_rate$Day, df = 16), "knots"),
  predict(fit_ns_2,
          data.frame(Day = attr(ns(mortality_rate$Day, df = 16), "knots"))),
  col="green",pch=4, lwd = 3, cex = 1)

plot(mortality_rate$Day, mortality_rate$lmr, xlab = 'Day', ylab = 'LMR',
     main = 'Natural cubic splies fit #4 (knots = 50)')
lines(mortality_rate$Day, pred4, col='red', lwd = 2)
points(attr(ns(mortality_rate$Day, df = 51), "knots"),
  predict(fit_ns_2,
          data.frame(Day = attr(ns(mortality_rate$Day, df = 51), "knots"))),
  col="red",pch=4, lwd = 3, cex = 1)


for (i in 1:4) {
  cat(sprintf("Model #%s MSE: %.2f \n", i,
              mean(eval(as.name(paste(c('fit_ns_', i), collapse = '')))$residuals^2)))  
}
```
<br>> Based on MSE model number four performs best. However with that many knots it can lead to overfit of the model. Model number three or even number two could be better choices. Needs more testing/validation.

##### 1.c
```{r, warning=F}

# Splitting data function
make_sample = function(data, seed_num, train_size) {
  set.seed(seed_num)
  smp_size <- floor(train_size * nrow(mtcars))
  train_ind <- sample(seq_len(nrow(mtcars)), size = smp_size)
  
  return_list = vector(mode="list", length=2)
  names(return_list) = c('train', 'test')
  
  train <- mortality_rate[train_ind, ]
  test <- mortality_rate[-train_ind, ]
  
  return_list[[1]] = train
  return_list[[2]] = test
  
  return(return_list)
}

# Calc MSE
calc_mse = function(actual, pred) {
  return(mean(actual - pred)**2)
}



test = function(data, nknots) {
  mse_data = data.frame(matrix(nrow = 0, ncol = 4))
  
  # Column names based on the knots
  cnames = c()
  cnames_mse = c()
  for (x in 1:length(nknots)) {
    cnames[x] = paste(c('nkonts_', nknots[x]), collapse ='')
    cnames_mse[x]= paste(c('MSE_of_nkonts_', nknots[x]), collapse ='')
  }
  colnames(mse_data) = cnames
  
  
  # Fitting and data splitting
  for (i in 1:10) {
    model_data = make_sample(data, 123 + i, 0.7)
  
    train = model_data$train
    test = model_data$test
      
    row_name = paste(c('MSE_', i), collapse = '')
    
    for (j in 1:length(nknots)) {
      fit = lm(lmr ~ ns(Day, df = nknots[j] + 1), data = train) # Hardcoded
      pred = predict(fit, test)
      
      # Warning: prediction from a rank-deficient fit may be misleading
      
      # A matrix that does not have "full rank" is said to be "rank deficient".
      # A matrix is said to have full rank if its rank is either equal to its
      # number of columns or to its number of rows (or to both).
      
      mse_data[i, j] = calc_mse(test$lmr, pred)
    }
  }

  
  # Return data
  return_data = vector(mode="list", length=5)
  
  names(return_data) = c('MSE_data', cnames_mse)
  
  return_data$MSE_data = mse_data
  for (c in 1:ncol(mse_data)) {
    return_data[c + 1] = mean(mse_data[, c])
  }
  
  return(return_data)
}


print(test(mortality_rate, nknots=c(1, 2, 15, 50)))

```
<br>
> Compared to 1 b MSEs are considerably higher. With this seed best MSE (1.56) is from model with fifthteen knots. Model with fifty knots gives absurd MSE.

##### 1.d. The final task of this assignment is to fit smoothing splines using the smooth.spline() function in R. Use generalized cross-validation to find the optimal degree of smoothing on each of the training data. Provide a plot of the data points and fitted spline curve for the first training data. Also, present the average of the estimated effective degrees of freedom and the smoothing parameter λ, as well as the number of proper knots. Compare the average predicted MSE with the best MSE from 1.c.

```{r}
data = make_sample(mortality_rate, 2707, 0.7)

train = data$train
test = data$test

smooth <- smooth.spline(x=train$Day, y=train$lmr, cv = T)
pred = stats:::predict.smooth.spline(smooth, test$Day)

cat(sprintf('MSE of the best lambda: %.2g \n', calc_mse(test$lmr, pred$y)))
cat(sprintf('Lambda: %.2g \n', smooth$lambda))  #Smoothing parameter
cat(sprintf('Number of Knots: %s \n', smooth$fit$nk))
cat(sprintf('Avarage of the effective Degrees of Freedom: %.3g \n', smooth$df - 2))


```
<br>
> Smoothing splines performs effectively comprored to natural cubic splines with MSE of 0.055 and difference to best cubic models MSE is (1.555 - 0.055) 1.05.


### Assignment 2: Comparison of GAMs with spline and loess basis functions

##### Pre

```{r}
auto_mpg = read.table('auto-mpg.txt', header = T)

auto_mpg$year = factor(auto_mpg$year)
auto_mpg$origin = factor(auto_mpg$origin)

auto_mpg = auto_mpg[ , !(names(auto_mpg) %in% 'name')]

summary(auto_mpg)
```

```{r, cache=TRUE}
# 2.a. Set up a lasso model (option ‘glmnet’) for all data with the train()
# function in the caret and use LOOCV to find the RMSE.
# In order to tune the alpha and lambda parameters, set the tuneLength = 50.
# Present the best model (not all) and its model fit criteria.

library(caret)

tuneGrid <- expand.grid(alpha = 1, lambda = seq(0.0001, 1, length = 50))

fit_lasso = train(mpg ~.,
                  data = auto_mpg,
                  method = "glmnet",
                  tuneGrid = tuneGrid,
                  tuneLength = 50,
                  trControl = trainControl("LOOCV", number = 1))


# Does not work
#fit_lasso$finalModel
getTrainPerf(fit_lasso)[1]

```
<br>
> How I can get the best model? fit_lasso$finalModel gives strange output

```{r, cache=TRUE}
# 2.b. Set up a GAM model with spline basis functions for all data
# with the train() function in the caret package and use LOOCV to find the best RMSE.
# Find a reasonable interval for the tuning parameter df. Present the table
# of the evaluated models and their model fit criteria.

library(gam)
# Build the GAM model based on splines and find best model with LOOCV
fit_gam_spline <- train(
  mpg~., data = auto_mpg, method = "gamSpline",
  scale = FALSE,
  trControl = trainControl("LOOCV", number = 1),
  tuneGrid = expand.grid(df = seq(1, 9, length = 9)))
```
```{r}
fit_gam_spline
#summary(fit_gam_spline$finalModel)
cat(sprintf('Best model RMSE: %.4g', min(fit_gam_spline$results$RMSE)))
```


##### 2.c
```{r, warning=F, cache=TRUE}
# 2.c. Set up a GAM model with loess basis functions for all data with the train()
# function in the caret package and use LOOCV to find the RMSE. Find a reasonable
# interval for the tuning parameter span and fix degree to 1.
# Present the table of the evaluated models and their model fit criteria.

# Build the GAM model based on splines and find best model with LOOCV
fit_gam_loess <- train(
  mpg~., data = auto_mpg, method = "gamLoess",
  scale = FALSE,
  trControl = trainControl("LOOCV", number = 1),
  tuneGrid = expand.grid(span = seq(0.1, 0.5, length = 9),
                         degree = c(rep(1,9))))
```
```{r}
fit_gam_loess
cat(sprintf('RMSE: %.4g', min(fit_gam_loess$results$RMSE)))
```


##### 2.d. Best Model

```{r, results='hold'}
# Present a plot of the regression coefficient/curves for the method that performed best.

# Best model is Generalized Additive Model using Splines 

par(mfrow = c(3,4))
plot(fit_gam_spline$finalModel)

```


