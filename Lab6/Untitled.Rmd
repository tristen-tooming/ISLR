---
title: "Lab6"
author: "Tristen Tooming"
date: "2/24/2021"
output: html_document
---

## Assignment 1: Investigate the prediction properties of Project Pursuit Regression, Multi Layer Perceptron, Convolutional Neural Network and Recurrent Neural Network.

In Lab 2_2 and 4_2, you investigated how PCR, PLS, Lasso, Ridge regression and Random Forest performed on the “tecator.csv” data which contains the results of a study aimed to investigate whether a near infrared absorbance spectrum can be used to predict the fat content of samples of meat. This data consists of highly correlated predictor sequences with an associated quantitative measurement. Your task is to implement and compare the prediction properties of the PPR using the ppr function in caret and MLP, 1dCNN and a RNN (LSTM) in Keras.

```{r Tecator-Data}
tecator = read.csv("/Users/tuuba/code/ISRL/Lab2/tecator.csv")
tecator_data = tecator[ , !names(tecator) %in% c('Sample', 'Protein', 'Moisture')]
```

#### 1.a.

Set up a PPR model for all data with the train() function in the caret package and use LOOCV to find the best RMSE. Find a reasonable interval for the tuning parameter nterms. Present the table of the evaluated models and their model fit criteria.

```{r, cache=TRUE, message=FALSE}
library(caret)
# getModelInfo("ppr")[[1]]$grid
# Build the Project Pursuit Regression model and find best # model with LOOCV tuning the nterms parameter
ppr_model <- train(
  Fat~., data = tecator_data, method = "ppr",
  trControl = trainControl("LOOCV"),
  tuneGrid = expand.grid(nterms = seq(1, 5, length = 5)))
```

```{r PPR-stats, results='hold'}
plot(ppr_model)

ppr_MSE = min(ppr_model$results$RMSE) ** 2
ppr_model
cat(sprintf("Best PPR MSE: %.3f", ppr_MSE))
```

#### 1.b.

Set up Keras models for the MLP, 1dCNN and a RNN (LSTM) following the web guide:
https://letyourmoneygrow.com/2018/05/27/classifying-time-series-with-keras-in-r-a-step-by-step-example/ 
Tune the models with the validation error and calculate test MSE for the full data following the lecture notes. Make sure that the validation error drops and converges to values around a minimum, but you need to take your own decision on what parameters to tune. Note that the LSTM model can be difficult to tune. Present the table of the evaluated models and their model fit criteria.

```{r, message=FALSE}
library(tensorflow)
tf$constant("Hellow Tensorflow")
```
##### MLP
```{r Deep-Learning-Model-Standartization}
library(keras)
library(dplyr)
library(tfdatasets)

# Comparison with OLS, scaled predictors
x<-tecator_data[, -1]
y<-tecator_data[, 1]
xsc<-scale(x)

# Data editing for Keras
col_names <- colnames(x)
colnames(x) <- NULL
train_df <- x %>% 
  as_tibble(.name_repair = "minimal") %>% 
  setNames(col_names) %>% 
  mutate(label = y)

# Standardization
spec <- feature_spec(tecator_data, Fat ~ . ) %>% 
  step_numeric_column(all_numeric(), normalizer_fn = scaler_standard()) %>% 
  fit()

```
```{r Deep-Learning-Multiple-Regression-Model}
input <- layer_input_from_dataset(tecator_data %>% select(-Fat))

outputlr <- input %>% 
  layer_dense_features(dense_features(spec)) %>% 
  layer_dense(units = 1,activation = 'linear', use_bias = T) # Output is 1 dimensional?
modellr <- keras_model(input, outputlr)

```

```{r Deep-Learning-Compile-Model, cache=T}
# Compile model 
modellr %>% 
   compile(
      loss = "mse",
      optimizer = optimizer_rmsprop(lr = 0.01), # Default 0.001
      metrics = list("mean_squared_error"))
# The patience parameter is the amount of epochs to check for improvement
early_stop_lr <- callback_early_stopping(monitor = "val_loss", patience = 50)
# Save best model to a file
save_best_lr <- callback_model_checkpoint(
  "best_model_lr.hdf5",
  monitor = "val_loss",
  verbose = 0,
  save_best_only = TRUE,
  mode = "min",
  save_freq = "epoch")
```

```{r Deep-Learning-Run-Model, cache=TRUE, message=F}
# Run the analysis
historylr <- modellr %>% fit(
  x = tecator_data %>% select(-Fat),
  y = tecator_data$Fat,
  epochs = 1000,
  validation_split = 0.2,
  verbose = 1,
  batch_size = 32,
  callbacks = list(early_stop_lr, save_best_lr)
)

```
```{r Deep-Learning-Model-Stats}
# Plot of validation loss
par(mfrow=c(1,2))
plot(historylr$metrics$val_loss,xlab="Epoch",
     ylab="Val_loss", type = 'l', col="blue")

plot(log(historylr$metrics$val_loss),xlab="Epoch",
     ylab="Log of Val_loss", type = 'l', col="blue")
# Best validation MSE
deep_mse = min(historylr$metrics$val_loss)

cat(sprintf("Min Deep MSE: %.3f", deep_mse))
```

##### RNN (LSTM)
```{r}
model_lstm <- keras_model_sequential()

model_lstm  %>%
  layer_lstm(units = 32, input_shape = c(215, 1), return_sequences = FALSE) %>% 
  layer_dense(units = 1, activation = 'linear') 

model_lstm %>% compile(
      loss = "mse",
      optimizer = optimizer_rmsprop(lr = 0.001), # Default 0.001
      metrics = list("mean_squared_error"))

model_lstm %>% summary()
```
```{r}
early_stop_lr <- callback_early_stopping(monitor = "val_loss", patience = 100)
# Save best model to a file
save_best_lr <- callback_model_checkpoint(
  "best_model_lstm.hdf5",
  monitor = "val_loss",
  verbose = 0,
  save_best_only = TRUE,
  mode = "min",
  save_freq = "epoch")
```

```{r Deep-Learning-Run-Model-LSTM, cache=TRUE, message=F}
# Run the analysis
tecator_X = array(as.numeric(unlist(tecator_data %>% select(-Fat))), dim = c(215, 100, 1))
tecator_y = tecator_data$Fat

# Somehow when doing in a loop just model parameters massed not history
# NVM just used wrong key. Too lazy to code loops again
history_5 <- model_lstm %>% fit(
              x = tecator_X,
              y = tecator_y,
              epochs = 2000,
              verbose = 1,
              batch_size = 5, 
              validation_split = 0.2,
              callbacks = list(early_stop_lr, save_best_lr))

history_10 <- model_lstm %>% fit(
              x = tecator_X,
              y = tecator_y,
              epochs = 2000,
              verbose = 1,
              batch_size = 10, 
              validation_split = 0.2,
              callbacks = list(early_stop_lr, save_best_lr))

history_20 <- model_lstm %>% fit(
              x = tecator_X,
              y = tecator_y,
              epochs = 2000,
              verbose = 1,
              batch_size = 20, 
              validation_split = 0.2,
              callbacks = list(early_stop_lr, save_best_lr))
```

```{r}
plot(history_5$metrics$val_loss, type = 'l', xlim = c(0, 200), ylim = c(0,100),
     main = 'Different batch sizes with LSTM',
     ylab = 'Val Loss')
lines(history_10$metrics$val_loss, col = 'blue')
lines(history_20$metrics$val_loss, col = 'red')
legend(150, 80, col = c('black', 'blue', 'red'),
       legend= c('5', '10', '20'), lty = 1)
```
```{r}
# Best model 20
# Best validation MSE
lstm_mse = min(history_20$metrics$val_loss)

cat(sprintf("Min LSTM MSE: %.3f", lstm_mse))
```

##### 1d CNN
```{r 1d-CNN, cache=TRUE, message=FALSE}
model_1d_cnn <- keras_model_sequential()

model_1d_cnn  %>%
  layer_conv_1d(filters=5, kernel_size=10,  activation = "relu",  input_shape=c(215, 1)) %>%
  layer_max_pooling_1d(pool_size = 4) %>%
  layer_flatten() %>% 
  layer_dense(units = 10, activation = 'relu')
  layer_dense(units = 1, activation = 'linear')

model_1d_cnn %>% compile(
      loss = "mse",
      optimizer = optimizer_rmsprop(lr = 0.001), # Default 0.001
      metrics = list("mean_squared_error"))

model_1d_cnn %>% summary()
```
```{r, cache=TRUE, message=FALSE}
history_1d_cnn <- model_lstm %>% fit(
              x = tecator_X,
              y = tecator_y,
              epochs = 2000,
              verbose = 1,
              batch_size = 32, 
              validation_split = 0.2,
              callbacks = list(early_stop_lr, save_best_lr))
```
```{r}
plot(history_1d_cnn$metrics$val_loss, type = 'l')
cnn_mse = min(history_1d_cnn$metrics$val_loss)

cat(sprintf("Min CNN MSE: %.3f", cnn_mse))
```



#### 1.c.

**How did the models perform in comparison with the earlier tried models?** 
```{r}
# Static values so my different from runs.
#Could use variables because they exist in the workspace across different files
mse_names = c('PPR', 'PLS', 'PCR', '1d CNN', 'LSTM', 'Lasso',
              'Ridge', 'MLP', 'RF')
mse_values = c(2.53, 3.92, 4.70, 1.73, 3.26, 12.42, 17.30, 17.47, 42.95)

mse_table = data.frame(Model = mse_names, MSE=mse_values)
mse_table = mse_table[order(mse_table$MSE), ]
knitr::kable(mse_table)
```
Overall 1d CNN performs best with MSE of 1.73, PPR was honorable number two with MSE of 2.53. Overall PPR was easiest to use of the methods in this exercise because the use was a no brainer. Neural Networks were a more hassle, took longer to configure and were and felt more like an art to tune. Universally Neural Networks took more time to get in a good shape but then again their performance were best. Personally I think that I didn't find optimal hyperparameters for the learning procedure and so on I didn't get "optimal" results.

If the time and computing power is your limitation I would firstly try less complex methods and move to Neural Netwroks if the model performance is not desirable. Also with Neural Networks sometimes you get MSE of 50 and other times MSE of 1.73 with same parameters. Is this due to bad learning hyperparameters?