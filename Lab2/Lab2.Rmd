---
title: "Lab2"
author: "Tristen Tooming"
date: "1/18/2021"
output:
  html_document:
    df_print: paged
    pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = '')
```
### 1

##### Pre

```{r}
library(ISLR)
library(leaps)

summary(Hitters)
```


##### Best Subset Selection
```{r}
# Fit
# Showing only best (nbest = 1)
regfit.models <- regsubsets(Salary~., data = Hitters, nbest = 1, nvmax = ncol(Hitters))

# Summary, Cp, BIC
res.sum <- summary(regfit.models)
# as.data.frame(res.sum$outmat)

plot(regfit.models, scale='Cp')
plot(regfit.models, scale='bic')

`````

```{r, echo=FALSE, results='hold'}
regfit.sum.min.cp = which.min(res.sum$cp)
regfit.sum.min.cp.value = min(res.sum$cp)
cat(sprintf('Best Cp (%.2f) model: %s \n', regfit.sum.min.cp.value, regfit.sum.min.cp))
cat(sprintf('Model: Salary ~ %s\n\n',
        paste(names(which(res.sum$which[regfit.sum.min.cp, ] == TRUE)),
              collapse = ' + ')))



regfit.sum.min.bic = which.min(res.sum$bic)
regfit.sum.min.bic.value = min(res.sum$bic)
cat(sprintf('Best BIC (%.2f) model: %s \n', regfit.sum.min.bic.value, regfit.sum.min.bic))
cat(sprintf('Model: Salary ~ %s\n\n',
            paste(names(which(res.sum$which[regfit.sum.min.bic, ] == TRUE)),
                  collapse = ' + ')))
```



##### Forward Stepwise Selection

```{r}
regfit.fwd = regsubsets(Salary~., data=Hitters, nvmax=ncol(Hitters), method ="forward")

regfit.fwd.sum = summary(regfit.fwd)

regfit.fwd.sum.min.bic = which.min(regfit.fwd.sum$bic)
regfit.fwd.sum.min.bic.value = min(regfit.fwd.sum$bic)

regfit.fwd.sum.min.cp = which.min(regfit.fwd.sum$cp)
regfit.fwd.sum.min.cp.value = min(regfit.fwd.sum$cp)

```
```{r, echo=FALSE, results='hold'}
cat(sprintf('\nBest Cp (%.2f) model: %s\n', regfit.fwd.sum.min.cp.value, regfit.fwd.sum.min.cp))
cat(sprintf('Model: Salary ~ %s\n\n',
            paste(names(which(regfit.fwd.sum$which[regfit.fwd.sum.min.cp, ] == TRUE)),
                  collapse = ' + ')))

cat(sprintf('Best BIC (%.2f) model: %s \n', regfit.fwd.sum.min.bic.value, regfit.fwd.sum.min.bic))
cat(sprintf('Model: Salary ~ %s\n\n',
            paste(names(which(regfit.fwd.sum$which[regfit.fwd.sum.min.bic, ] == TRUE)),
                  collapse = ' + ')))
```



##### Backward Stepwise Selection
```{r}
regfit.bfd = regsubsets(Salary~., data=Hitters, nvmax=ncol(Hitters), method ="backward")
regfit.bfd.sum = summary(regfit.bfd)

regfit.bfd.sum.min.bic = which.min(regfit.bfd.sum$bic)
regfit.bfd.sum.min.bic.value = min(regfit.bfd.sum$bic)

regfit.bfd.sum.min.cp = which.min(regfit.bfd.sum$cp)
regfit.bfd.sum.min.cp.value = min(regfit.bfd.sum$cp)

```
```{r, echo=FALSE, results='hold'}
cat(sprintf('\nBest Cp (%.2f) model: %s \n', regfit.bfd.sum.min.cp.value, regfit.bfd.sum.min.cp))
cat(sprintf('Model: Salary ~ %s\n\n',
            paste(names(which(regfit.bfd.sum$which[regfit.bfd.sum.min.cp, ] == TRUE)),
                  collapse = ' + ')))

cat(sprintf('Best BIC (%.2f) model: %s \n', regfit.bfd.sum.min.bic.value, regfit.bfd.sum.min.bic))
cat(sprintf('Model: Salary ~ %s\n\n',
            paste(names(which(regfit.bfd.sum$which[regfit.bfd.sum.min.bic, ] == TRUE)),
                  collapse = ' + ')))
```

##### Exercise 1. Conclusion 

```{r, rows.print=20, echo=FALSE}
library(broom)
full_model = lm(Salary ~ ., data = Hitters)
coefs = tidy(full_model)
coefs = coefs[order(coefs$p.value), ]
knitr::kable(coefs, caption = 'Coeffs and their Significance (Sorted by P-value)', digits = 3)
```


Best Subset Selection and Forward Stepwise Selection perform simillary and they give identical results. However selecting model based on BIC criteria Backward Stepwise Selection performs best with BIC value of -147.38 compared -147.92 when using Best Subset and Stepwise Forward methods. When using Cp-value as a comparison all methods gave identical models: Salary ~ (Intercept) + AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks + DivisionW + PutOuts + Assists with Cp-value of 5.01. 

Using p-values of the full model and just simply counting and concluding subset, forward and backwards results the most important variables are: PutOuts, Walks, Hits, AtBat, DivisionW, Cwalks, CRuns and AtBat. For improving model we could add interaction terms and perform model selection again. 

### 2

##### Pre

```{r, echo = FALSE, echo=F}
library(caret)
library(pls)
library(caTools)
library(MASS)
library(tidyverse)
```

```{r}
tecator = read.csv("/Users/tuuba/code/ISRL/Lab2/tecator.csv")
tecator_subset = tecator[ , !names(tecator) %in% c('Sample', 'Protein', 'Moisture')]

set.seed(2707)
smp_size <- floor(0.75 * nrow(tecator_subset))
train_ind <- sample(seq_len(nrow(tecator_subset)), size = smp_size)
tecator_train = tecator_subset[train_ind, ]
tecator_test = tecator_subset[-train_ind, ]

parcoord(tecator_subset)
```
> Conclusion: The straight lines indicate good linear relationships so on PCR, PLS, Lasso and Ridge ression could be suitable to model the data.

```{r}
model_pcr <- train(
  Fat~., data = tecator_train, method = "pcr",
  scale = TRUE, # scale = TRUE for standardizing the variables to make them comparable.
  trControl = trainControl("cv", number = 25),
  tuneLength = 50
  )
# Plot model RMSE vs different values of components
plot(model_pcr, main = 'Performance of PCR')
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
model_pcr$bestTune
```
```{r}
model_pls <- train(
  Fat~., data = tecator_train, method = "pls",
  scale = TRUE, # scale = TRUE for standardizing the variables to make them comparable.
  trControl = trainControl("cv", number = 25),
  tuneLength = 50
  )
# Plot model RMSE vs different values of components
plot(model_pls, main = 'Performance of PLS')
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
model_pls$bestTune
```
```{r}
# Predictions

# Make predictions
pcr_predictions <- model_pcr %>% predict(tecator_test)
pls_predictions <- model_pls %>% predict(tecator_test)
# Model performance metrics
pcr_mse = caret::RMSE(pcr_predictions, tecator_test$Fat)**2
pls_mse = caret::RMSE(pls_predictions, tecator_test$Fat)**2

```

```{r, warning=FALSE}
# Regularization parameters
parameters <- c(seq(0.1, 2, by =0.1) ,  seq(2, 5, 0.5) , seq(5, 25, 1))

model_lasso <- train(
  Fat~., data = tecator_train, method = "glmnet",
  scale = TRUE, # scale = TRUE for standardizing the variables to make them comparable.
  trControl = trainControl("cv", number = 25),
  tuneGrid = expand.grid(alpha = 1,  lambda = parameters),
  metric = 'RMSE'
  )

model_ridge <- train(
  Fat~., data = tecator_train, method = "glmnet",
  scale = TRUE, # scale = TRUE for standardizing the variables to make them comparable.
  trControl = trainControl("cv", number = 25),
  tuneGrid = expand.grid(alpha = 0,  lambda = parameters),
  metric = 'RMSE'
  )

plot(model_lasso, main = 'Performance of Lasso')
plot(model_ridge, main = 'Performance of Ridge')

# Predictions 
lasso_predictions <- model_lasso %>% predict(tecator_test)
ridge_predictions <- model_ridge %>% predict(tecator_test)

# Model performance metrics
lasso_mse = caret::RMSE(lasso_predictions, tecator_test$Fat)**2
ridge_mse = caret::RMSE(ridge_predictions, tecator_test$Fat)**2
```
```{r, echo=FALSE, results='hold'}
cat(sprintf("PCR MSE: %.2f\n", pcr_mse))
cat(sprintf("PLS MSE: %.2f\n", pls_mse))
cat(sprintf("Lasso MSE: %.2f\n", lasso_mse))
cat(sprintf("Ridge MSE: %.2f\n\n", ridge_mse))
```

PLS has the best performance with MSE of 3.92, then comes PCR (MSE = 4.70), Lasso (MSE = 12.42) and lastly Ridge (MSE = 17.30). 



























